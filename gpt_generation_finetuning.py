import os
import sys
import torch
import argparse
from datasets import load_dataset
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments

def tokenize_imdb_reviews(dataset, tokenizer, max_length=150):
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    def tokenize_function(examples):
        return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=max_length)
    tokenized = dataset.map(tokenize_function, batched=True)
    tokenized = tokenized.rename_column("label", "labels")
    return tokenized

def train_sentiment_model(
    imdb_subset_path,
    sentiment_label,
    output_dir,
    max_length=150,
    num_epochs=3,
    batch_size=4
):
    dataset = load_dataset("json", data_files=imdb_subset_path, split="train")
    filtered_dataset = dataset.filter(lambda x: x["label"] == sentiment_label).select(range(100))  # 100 samples
    model = GPT2LMHeadModel.from_pretrained("gpt2")
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    tokenized_data = tokenize_imdb_reviews(filtered_dataset, tokenizer, max_length=max_length)
    collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=num_epochs,
        per_device_train_batch_size=batch_size,
        save_steps=10000,
        save_total_limit=2,
        logging_steps=50,
        logging_dir=os.path.join(output_dir, "logs"),
        report_to="none"
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_data,
        data_collator=collator
    )
    trainer.train()
    trainer.model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    return output_dir

def generate_reviews(model_dir, prompt="The movie was", num_reviews=5):
    tokenizer = GPT2Tokenizer.from_pretrained(model_dir)
    model = GPT2LMHeadModel.from_pretrained(model_dir)
    model.eval()
    reviews_list = []
    input_ids = tokenizer.encode(prompt, return_tensors="pt")

    # keeps max_length at 30, but tries to finish earlier if sensible
    # note that this won't always guarantee a perfect sentence ending
    with torch.no_grad():
        for _ in range(num_reviews):
            output = model.generate(
                input_ids,
                min_length=10,
                max_length=30,
                temperature=0.75,
                do_sample=True,
                top_k=50,
                top_p=0.95,
                repetition_penalty=1.2,
                early_stopping=True
            )
            text = tokenizer.decode(output[0], skip_special_tokens=True)
            reviews_list.append(text)
    return reviews_list

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("imdb_subset_path", type=str, help="path to the imdb subset json file")
    parser.add_argument("generated_reviews_path", type=str, help="path to save generated reviews txt file")
    parser.add_argument("models_output_dir", type=str, help="path to save fine-tuned models")
    args = parser.parse_args()

    os.makedirs(args.models_output_dir, exist_ok=True)
    positive_model_dir = os.path.join(args.models_output_dir, "gpt2_positive")
    negative_model_dir = os.path.join(args.models_output_dir, "gpt2_negative")

    print("training positive model...")
    train_sentiment_model(
        imdb_subset_path=args.imdb_subset_path,
        sentiment_label=1,
        output_dir=positive_model_dir
    )

    print("training negative model...")
    train_sentiment_model(
        imdb_subset_path=args.imdb_subset_path,
        sentiment_label=0,
        output_dir=negative_model_dir
    )

    print("generating reviews...")
    positive_reviews = generate_reviews(positive_model_dir, prompt="The movie was", num_reviews=5)
    negative_reviews = generate_reviews(negative_model_dir, prompt="The movie was", num_reviews=5)

    with open(args.generated_reviews_path, "w", encoding="utf-8") as f:
        f.write("reviews generated by positive model:\n")
        for idx, rev in enumerate(positive_reviews, 1):
            f.write(f"{idx}. {rev}\n\n")
        f.write("\nreviews generated by negative model:\n")
        for idx, rev in enumerate(negative_reviews, 1):
            f.write(f"{idx}. {rev}\n\n")

    print("done.")

if __name__ == "__main__":
    main()
